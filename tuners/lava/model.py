import torch
import torch.nn as nn
import re

from peft.tuners.tuners_utils import BaseTuner
from peft.tuners.tuners_utils import check_target_module_exists
from peft.tuners.lava.layer import LavaLayer
from peft.tuners.lava.config import LavaConfig


class LavaModel(BaseTuner):
    """
    LoRAì™€ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ë”°ë¥´ëŠ” VAE-style Linear Adapter Model.
    BaseTunerë¥¼ ê·¸ëŒ€ë¡œ í™•ì¥í•˜ì—¬
    - adapter injection
    - enabling/disabling adapters
    - trainable param marking
    ëª¨ë‘ PEFTì™€ í˜¸í™˜ë˜ë„ë¡ êµ¬í˜„.
    """

    # ------------------------------------------------------------
    # 1. __init__
    # ------------------------------------------------------------
    def __init__(self, model: nn.Module, config: LavaConfig, adapter_name: str, low_cpu_mem_usage=False):
        super().__init__(model, config, adapter_name, low_cpu_mem_usage)
        # BaseTunerê°€ ëª¨ë“  injection ë¡œì§ì„ ì‹¤í–‰í•¨.
        self.config = model.config          # <- Trainerê°€ ìš”êµ¬í•¨
        self.peft_config = config           # <- PEFT ë‚´ë¶€ êµ¬ì¡° ìœ ì§€
    # ------------------------------------------------------------
    # 2. adapter switching
    # ------------------------------------------------------------
    def set_adapter(self, adapter_names):
        """
        BaseTunerModel â†’ BaseTunerLayer êµ¬ì¡°ì™€ ë™ì¼í•˜ê²Œ,
        ê° LavaLayerì— adapter ì´ë¦„ì„ ì „ë‹¬.
        """

        # adapter_names ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ í†µì¼
        if isinstance(adapter_names, str):
            adapter_names = [adapter_names]

        # LavaLayer ì— ì „ë‹¬
        for module in self.model.modules():
            if isinstance(module, LavaLayer):
                module.set_adapter(adapter_names)

        # BaseTuner ë‚´ë¶€ì—ì„œ active adapterëŠ” ì—¬ê¸°ì— ì €ì¥ë¨
        self._active_adapter = adapter_names
        
    def enable_input_require_grads(self, **kwargs):
        """
        Gradient Checkpointingì„ ìœ„í•´ ì…ë ¥ ì„ë² ë”©ì— gradë¥¼ ìš”êµ¬í•˜ëŠ” ë©”ì„œë“œë¥¼ 
        ë‚´ë¶€ì˜ ì‹¤ì œ ëª¨ë¸(Llama ë“±)ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        """
        if hasattr(self.model, "enable_input_require_grads"):
            return self.model.enable_input_require_grads(**kwargs)
        else:
            # ì§ì ‘ êµ¬í˜„ (ìœ„ ë©”ì„œë“œê°€ ì—†ëŠ” êµ¬í˜• ëª¨ë¸ ëŒ€ë¹„)
            def make_inputs_require_grad(module, input, output):
                output.requires_grad_(True)
            return self.model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
    # ------------------------------------------------------------
    # 3. adapter config ì¤€ë¹„
    # ------------------------------------------------------------
    def _prepare_adapter_config(self, peft_config: LavaConfig, model_config):
        """
        LoRA êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¦„.
        target_modules ê¸°ë³¸ê°’ ì„¤ì •.
        """

        if peft_config.target_modules is None:
            # ê¸°ë³¸ì ìœ¼ë¡œ transformer QKV projection linear ì— ì ìš©
            peft_config.target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

        if peft_config.r is None:
            peft_config.r = 8

        return peft_config

    # ------------------------------------------------------------
    # 4. ì–´ë–¤ ëª¨ë“ˆ ì´ë¦„ì´ ëŒ€ìƒì¸ì§€ ì²´í¬
    # ------------------------------------------------------------
    def _check_target_module_exists(self, peft_config: LavaConfig, key: str):
        return check_target_module_exists(peft_config, key)

    # ------------------------------------------------------------
    # 5. injection â€” Linear ë¥¼ LavaLayer ë¡œ êµì²´
    # ------------------------------------------------------------
    def _create_and_replace(
        self,
        peft_config: LavaConfig,
        adapter_name: str,
        target: nn.Module,
        target_name: str,
        parent: nn.Module,
        current_key: str,
    ):
        """
        LoRA êµ¬ì¡°ì™€ ë™ì¼í•˜ê²Œ Linearë§Œ êµì²´.
        """

        # Linear ê°€ ì•„ë‹ˆë©´ skip
        if not isinstance(target, nn.Linear):
            return

        # âœ… ë ˆì´ì–´ ì¸ë±ìŠ¤ ì¶”ì¶œ
        layer_idx = 0
        match = re.search(r'\.(?:layers|block|h)\.(\d+)\.', current_key)
        if match:
            layer_idx = int(match.group(1))

        # r ê°€ì ¸ì˜¤ê¸°
        rank = peft_config.r
        alpha = peft_config.alpha
        lora_dropout = getattr(peft_config, 'lora_dropout', 0.0)

        # ìƒˆë¡œìš´ LavaLayer ìƒì„±
        new_module = LavaLayer(
            base_layer=target,
            adapter_name=adapter_name,
            rank=rank,
            alpha=alpha,
            lora_dropout=lora_dropout,
        )

        # parent ëª¨ë“ˆì— êµì²´ ë°˜ì˜
        setattr(parent, target_name, new_module)

    # ------------------------------------------------------------
    # 6. trainable parameter ì„¤ì •
    # ------------------------------------------------------------
    def _mark_only_adapters_as_trainable(self, model):
        """
        Lava adapterë§Œ í•™ìŠµë˜ë„ë¡ ì„¤ì •.
        """
        for name, param in model.named_parameters():
            if "lava" in name:   # LavaLayer ë‚´ë¶€ adapter í‚¤ì›Œë“œ
                param.requires_grad = True
            else:
                param.requires_grad = False

    # ------------------------------------------------------------
    # 7. enable / disable adapters
    # ------------------------------------------------------------
    def enable_adapter_layers(self):
        for module in self.model.modules():
            if isinstance(module, LavaLayer):
                module.disable_adapters = False

    def disable_adapter_layers(self):
        for module in self.model.modules():
            if isinstance(module, LavaLayer):
                module.disable_adapters = True

    # ------------------------------------------------------------
    # 8. forward
    # ------------------------------------------------------------
    def forward(self, *args, **kwargs):
        """
        BaseTuner â†’ BaseModel forward í˜¸ì¶œ
        ViT ê°™ì€ ì´ë¯¸ì§€ ëª¨ë¸ì€ input_idsë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°
        """
        kwargs["return_dict"] = True
        # ViT ë“± ì´ë¯¸ì§€ ëª¨ë¸ í˜¸í™˜ì„±: input_ids, attention_mask, inputs_embeds ì œê±°
        if "pixel_values" in kwargs:
            kwargs.pop("input_ids", None)
            kwargs.pop("attention_mask", None)
            kwargs.pop("inputs_embeds", None)
        return self.model(*args, **kwargs)
    
        
        
    def gradient_checkpointing_enable(self, **kwargs):
        """
        Trainer compatibility:
        forward gradient checkpointing call to base model
        """
        if hasattr(self.model, "gradient_checkpointing_enable"):
            return self.model.gradient_checkpointing_enable(**kwargs)

    def gradient_checkpointing_disable(self):
        if hasattr(self.model, "gradient_checkpointing_disable"):
            return self.model.gradient_checkpointing_disable()

    @property
    def is_gradient_checkpointing(self):
        if hasattr(self.model, "is_gradient_checkpointing"):
            return self.model.is_gradient_checkpointing
        return False
    # ------------------------------------------------------------
    # 9. generation interface (ğŸ”¥ í•„ìˆ˜)
    # ------------------------------------------------------------
    def prepare_inputs_for_generation(self, *args, **kwargs):
        return self.model.prepare_inputs_for_generation(*args, **kwargs)

    # (optional but recommended)
    def generate(self, *args, **kwargs):
        return self.model.generate(*args, **kwargs)

    def get_input_embeddings(self):
        return self.model.get_input_embeddings()

    def get_output_embeddings(self):
        return self.model.get_output_embeddings()